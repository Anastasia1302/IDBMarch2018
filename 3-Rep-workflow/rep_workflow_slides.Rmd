---
title: "Reproducible Workflow: Coding Strategies and Software [75mins]"
subtitle: "Introduction, Hands-on with Version Control (Github) and Dynamic Documents (RMarkdown)"  
author: "| Fernando Hoces de la Guardia\n| BITSS   \n| -\n| Slides at <https://goo.gl/aBQ3LR>\n"
date: "Inter-American Development Bank Workshop, March 2018"
output: 
  beamer_presentation:
    slide_level: 2
---
# File Management & Coding Suggestions (Christensen et al, 2018) [15 mins]
## File Management [5 mins] 
![](files.png)

## Organizing Principles

### 1 - Use code (scripts), don't work by hand (GUI's or comand line).
### 2 - Consider not saving statistical output, and just saving the code and data that generates it.
### 3 - Reproducibility. Minimum: machine (laptop) independence. Ideal: analyst indepence. 

## Coding Suggestions [10 mins]

<!--
1)	Make sure that your script files are self-contained. That is, don't write a program that only works if you run a group of other files previously in a specific order and then leave things hanging in a certain precarious way. You should be able to run intermediate steps of the workflow without disrupting things downstream.  
-->  
1) Include tests in your code.   
<!-- 
This can alert you if output ever changes unexpectedly. For example, if merging intermediate datasets and dropping unmatched observations, you could write code to throw an error and alert you if the number of observations changes. (See the Stata example below.)
-->
2) You can never comment your code too much.   
<!-- Comments should truly explain what the code is doing rather than merely transliterating: it is more useful to describe x<-1 with "initialize the population count to 1" than "set x equal to 1." Comments should also be checked to make sure they don't go out of date and convey inaccurate information.
--> 
3) Indent your code.   
4) Once posted, any changes at all require a new file name. Or a version control system in place.  
5) Separate your data cleaning and analysis files
<!-- don't make any new variables that need saving (or will be used by multiple analysis files) in an analysis file. It is far better to only create a variable once so you know that it is identical when used in different analysis files.
-->  
6) Never name a file "final" because it won't be.
<!-- , and why do you want to tempt fate? Instead, add a date and author initials or version number.
-->  
7) Name binary variables "male" instead of "gender," (1=Male and 0=Not)

8) Don't leave clutter around-delete temporary or unnecessary intermediate objects. 
<!-- You can use a prefix such as x_ or temp_ so you know which files or variables can easily be deleted later. Stata also has the tempfile and tempvar functionality.  
-->  
9) Every variable should have a label. 

10) Use relative directory paths (such as "./Data" and not "C:/Users/Fernando/Documents/Project/Data") 

## Coding Suggestions: Stata-specific
1)	Accurately and concisely capture missing values. (`.` and `.a-.z`)
<!-- There are multiple reasons that a value might be blank. To fully convey this, use the full set of missing values available to you (".a"-".z", not exclusively ".") in order to distinguish between "don't know" and "didn't ask," or other distinct reasons for missing data. 
-->
2)	Make sure code always produces the same result, and that merging and sorting is reproducible. `duplicates report; isid; sort, stable`  

3)	Run simple tests to alert yourself when results change. Example:

        count if _merge!=3
        if r(N)!=74 {
        display "Unmatched observations changed!"
        there is an error here
        } 
4)	Don't use abbreviations for variables or commands.
5)	Use global macros to define directory paths so collaborators can readily work across different computers.
6)	Use local macros for varlists.

## Coding Suggestions: Stata-specific
7)	Use computer-stored versions of numerical output (eg `r(mean)`). Use `return list` and `ereturn list`
8)	If you have a master .do file that calls other .do files, which each have their own .log file capturing output, you can run multiple log files at the same time (so you end up with a master .log file)
9)	Use the `label data` and `notes`.
10)	Use the `notes` command for variables as well for identifying information that is too long for the variable label.
 
11)	Validate data sources to ensure consistency. Use `datasignature` on auto data set (`sysuse auto.dta`, then `datasignature set` should give you this number: `74:12(71728):3831085005:1395876116`) 
12)	Use value labels for all categorical variables. `numlabel [lblname-list], add command.`  
13)	Don't use capital letters in variable names.
14)	Make your files as non-proprietary as possible (use the `saveold` command) 


# Version Control [30 mins]

## Problem to avoid
![http://www.phdcomics.com/comics/archive/phd101212s.gif](http://www.phdcomics.com/comics/archive/phd101212s.gif)

## Managing expectations
![Git xkcd comic](https://imgs.xkcd.com/comics/git.png)


# Dynamic Documents [30 mins]

```{r consort diagram, echo=FALSE}
#install.packages("DiagrammeRsvg")
# this codes comes from:
# https://scriptsandstatistics.wordpress.com/2017/12/22/how-to-draw-a-consort-flow-diagram-using-r-and-graphviz/
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)
library(webshot)

#webshot::install_phantomjs()


# Values ------------------------------------------------------------------
values <- c(210, 10, 200, 100, 100, 10, 10, 90, 90)
 
# Defining Text Labels ----------------------------------------------------
text <- c('Assessment for\neligibility',
          'Excluded',
          'Randomized',
          'Allocated to\nintervention',
          'Allocated to\nintervention',
          'Lost to follow-up',
          'Lost to follow-up',
          'Analysed',
          'Analysed')
 
# Defining Function -------------------------------------------------------
paste1 <- function(x, y){
  paste0(x, ' (n=', y, ')')
}
 
# Concatenating Values and Text Labels ------------------------------------
LABS <- paste1(text, values)


ndf <-
  create_node_df(
    n = 21,
    label = c('Enrollment', 'Allocation', 'Follow-Up', 'Analysis',
              LABS, rep("", 8)),
    style = c(rep("solid", 13), rep('invis', 8)),
    shape = c(rep("plaintext", 4), 
              rep("box", 9),
              rep("point", 8)),
    width = c(rep(2, 4), rep(2.5, 9), rep(0.001, 8)),
    hight = c(rep(0.5, 13), rep(0.001, 8)),
    fontsize = c(rep(14, 4), rep(10, 17)),
    fontname = c(rep('Arial Rounded MT Bold', 4), rep('Courier New', 17)),
    penwidth = 2.0,
    fixedsize = "true")

edf <-
  create_edge_df(
    arrowhead = c(rep('none', 3), rep("vee", 3), rep('none', 2), "vee", rep('none', 6),
                  rep("vee", 3), rep("none", 3), "vee", rep("none", 10)),
    color = c(rep('#00000000', 3), rep('black', 6), rep('#00000000', 6),
              rep('black', 3), rep('#00000000', 3), rep('black', 1),
              rep('#00000000', 2), rep('black', 2), 
              rep('#00000000', 6)),
    constraint = c(rep("true", 18), rep('false', 14)),
    from = c(1, 19, 20, 16, 8, 10, # column 1
             5, 14, 7, 15, 2, 3, # column 2
             18, 6, 21, 17, 9, 11, # column 3
             1, 5, # row 1
             19, 14, # row 2
             20, 7, # row 3
             16, 15, # row 4
             8, 2, # row 5
             10, 3, # row 6
             12, 4), # row 7
    to = c(19, 20, 16, 8, 10, 12, # column 1
           14, 7, 15, 2, 3, 4, # column 2
           6, 21, 17, 9, 11, 13, # column 3
           5, 18, # row 1
           14, 6, # row 2
           7, 21, # row 3
           15, 17, # row 4
           2, 9, # row 5
           3, 11, # row 6
           4, 13)) # row 7

# Create Graph ------------------------------------------------------------
g <- create_graph(ndf, 
                  edf,
                  attr_theme = NULL)
 

```
## CONSORT diagram of our little experiment
```{r, echo=FALSE}
# Plotting ----------------------------------------------------------------
render_graph(g)

```

## Balance of covariates 

## Estimated effect

## The Stata version of all of the above: 
